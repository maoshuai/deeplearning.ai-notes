


# Welcome to Deeplearning Specialization

## welcome

æ·±åº¦å­¦ä¹ çš„å…‰æ˜å‰æ™¯ï¼š

> AI is the new Electricity!

5ä¸ªCourseçš„ç®€ä»‹ï¼Œä»¥åŠCourse1çš„ç›®æ ‡ï¼šRecognize Cat

å‡ ä¸ªå¦‚é›·è´¯è€³çš„åè¯ï¼ˆå…ˆè®°ä¸‹ï¼Œåé¢æ…¢æ…¢å­¦ï¼‰ï¼‰ï¼š
* CNN: Convolutional Neural Networks å·ç§¯ç¥ç»ç½‘ç»œ
* RNN: Recurrent Neural Networks å¾ªç¯ç¥ç»ç½‘ç»œ
* LSTM: Long Short-Term Memory é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ

# Introdcution to Deep Learning
## what is a neural network?

Deep Learning = training (very large) neural network

What's a neural network?
* a very **simple** neural network: House Price-size Linear regressionï¼ˆå°±åƒä¸€ä¸ªLego brickï¼‰
![simple-neural-network](/content/images/2018/05/simple-neural-network.png)
* a **larger** neural network: stacking together many of these Lego bricksã€‚
![manual-larger-neural-network](/content/images/2018/05/manual-larger-neural-network.png)

æ³¨æ„ï¼Œä¸Šå›¾åªæ˜¯ä¸€ä¸ªæ‰‹å·¥æ¼”ç¤ºçš„larger neural networkï¼Œä¸å®é™…çš„åŒºåˆ«ï¼š
1. hidden layerå¹¶ä¸æ˜¯æ‰‹å·¥è®¾ç½®çš„
2. æ¯ä¸€å±‚éƒ½æ˜¯ä¸Šä¸€å±‚æ‰€æœ‰è¾“å…¥çš„å‡½æ•°

å®é™…çš„neural networkï¼š
![real-neural-network](/content/images/2018/05/real-neural-network.png)

Neural Networkçš„ä¸¤ä¸ªç‰¹ç‚¹ï¼›
1. Neural networks are remarkably good at **figuring out functions that accurately map from x to y**. 
2. Most powerful in supervised learning.

ä¸€ä¸ªå¸¸è§çš„æ¿€æ´»å‡½æ•° ReLU ï¼š
ReLU function: **Re**ctified **Li**near **U**nits (rectified means: taking a min of zeroï¼ŒåŸæ–‡æ˜¯taking a *max* of zeroï¼Œæˆ‘è§‰å¾—å¯èƒ½è®²åäº†å§)
![relu](/content/images/2018/05/relu.png)

## Supervised Learning with Neural Network

æˆªæ­¢åˆ°ç›®å‰ï¼Œ**Neural Networkçš„æˆåŠŸåº”ç”¨åŸºæœ¬éƒ½åœ¨Supervised Learning**ã€‚æ¯”å¦‚ï¼šAdï¼ŒImages vision, Audio to Text, Machine translation, Autonomous Driving![supervised-learning-exmples](/content/images/2018/05/supervised-learning-exmples.png)


Neural Network examples:
![](/content/images/2018/07/NeuralNetworkExamples.jpg)

Structured Data vs Unstructured Data

## Why is Deeplearning taking off? 

**Scale** drives deep learning progress
![Screen-Shot-2018-05-28-at-20.42.54](/content/images/2018/05/Screen-Shot-2018-05-28-at-20.42.54.jpg)
Scale means: Bigger network and More (labeled) data.

åœ¨æ•°æ®åŒ®ä¹çš„æ—¶ä»£ï¼Œç®—æ³•çš„æ€§èƒ½æ›´ä¾èµ–äºæŠ€å·§å’Œæ‰‹å·¥è®¾ç½®çš„feature (the algorithms is actually not very well defined so if you don't have a lot of training data is often up to your skill at hand engineering features )
å¤§æ•°æ®æ—¶ä»£ï¼Œå¤§æ•°æ®å¤„äºæ”¯é…åœ°ä½ã€‚(big data regime very large training sets very large M regime in the right that we more consistently see largely Ronettes dominating the other approaches)

ä¸‰å¤§åŸå› ï¼š
* Data
* Computation
æ¯”å¦‚ï¼šGPU
* Algorithm
æ¯”å¦‚ï¼šsigmoid functionâ†’ReLU function
![Screen-Shot-2018-05-28-at-20.53.10](/content/images/2018/05/Screen-Shot-2018-05-28-at-20.53.10.jpg)

ç®—æ³•å¿…é¡»å¿«ï¼Œå½¢æˆä¸€ä¸ªæ­£å¾ªç¯ï¼Œå¿«é€ŸéªŒè¯ï¼š
![Screen-Shot-2018-05-28-at-20.54.59](/content/images/2018/05/Screen-Shot-2018-05-28-at-20.54.59.jpg)


# Hero of Deep Learning
As part of this course by deeplearning.ai, hope to not just teach you the technical ideas in deep learning, but also **introduce you to some of the people, some of the heroes in deep learning**.ï¼ˆå¤§æ¦‚çœ‹äº†ä¸€ä¸‹ï¼Œåé¢è¿˜ä¼šä»‹ç»å¾ˆå¤šå¤§ç¥ï¼Œè¿™æ˜¯æ¯”ä¹‹å‰çš„Machine Learningå¢åŠ çš„ç‰¹è‰²ï¼‰

Geoffrey Hinton interview
![Geoffrey Hinton interview](/content/images/2018/05/Screen-Shot-2018-05-28-at-20.59.23.jpg)

* God father of deep learning
* ä¼ å¥‡è¾—è½¬çš„æ±‚å­¦ç»å†ï¼Œä¸ºç ”ç©¶how does the brain store memories.ï¼š
    * Hologram made me interested in how does the **brain store memories**. 
    * Studying physiology and physics in Cambridege. 
    * Gave up and tried to do philosophy
    * Switched to psychology. 
    * Took some time off and became a carpenterï¼ˆçæŠ˜è…¾è¿™ä¹ˆä¹…ï¼Œä¹Ÿè¿·èŒ«äº†ğŸ˜„ï¼‰
    * Then I decided that I'd try AI, and went of to Edinburgh
    * Then get a PhD in AI.
* å‰æ™¯æƒ¨æ·¡
    * æ‰¾ä¸åˆ°å·¥ä½œ
    * in Britain, neural nets was regarded as kind of silly
* çœ‹åˆ°æ›™å…‰
    * in California, Don Norman and David Rumelhart were very open to ideas about neural nets. **It was the first time I'd been somewhere where thinking about how the brain works**ï¼ˆä¸å¿˜åˆå¿ƒå•Šï¼‰, and thinking about how that might relate to psychology, was seen as a very positive thing. 
    * 1982ä¸Rumelhartå†™äº†backpropagationè®ºæ–‡, in UCSDã€‚è€Œå®é™…ä¸Šè¿™åˆæ˜¯ä¸€ä¸ªé‡å¤å‘æ˜äº†å¾ˆå¤šæ¬¡çš„ç®—æ³•ã€‚ä½†ç›´åˆ°ä»–ä»¬çš„å·¥ä½œï¼Œbackpropagationæ‰çœŸæ­£è¢«äººä»¬é‡è§†ã€‚
* æœ€å¾—æ„çš„ç®—æ³•å‘æ˜ï¼šSo I think the most beautiful one is the work I do with Terry Sejnowski on **Boltzmann machines**ã€‚
    *  **Simple** algorithm, beautiful.
    *  And it looked like the kind of thing you should be able to get in a brain because each synapse only needed to know about the behavior of the two neurons it was directly connected to. 
*  ReLU
*  Relationship between backpropagation and brains
    *  I guess my main thought is this. If it turns out the back prop is a really good algorithm for doing learning. Then for sure evolution could've figured out how to prevent(å­—å¹•å¯èƒ½ç”¨é”™è¯äº†å§ï¼‰ it. 
    *  And I think the brain probably has something that may not be exactly be backpropagation, but it's quite close to it
* Multiple time skills in deep learning
    * Fast weights
* capsules
    * subsets as a capsule
* your understanding of AI changed over these years? 
    * most humain learning is unsupervised learning. What's worked over the last ten years or so is supervised learning, **in the long run, I think unsupervised learning is going to be absolutely crucial**.  And things will work incredibly much better than they do now when we get that working properly, but we haven't yet. 
    * I think generative adversarial nets are one of the sort of biggest ideas in deep learning that's really new. 
* a sort of basic principle about how you model anything. 
    * You take your measurements, and **you're applying nonlinear transformations to your measurements until you get to a representation as a state vector in which the action is linear**. So you don't just pretend it's linear like you do with common filters. But you actually find a transformation from the observables to the underlying variables where linear operations, like matrix multipliers on the underlying variables, will do the work. 
* Advice for breaking through AI and deeplearning
    * My advice is sort of read the literature, but don't read too much of it.Read a little bit of the literature. And notice something that you think everybody is doing wrong, I'm contrary in that sense. You look at it and it just doesn't feel right. And then figure out how to do it right. 
    * Develop and trust your intuitions. When you have what you think is a good idea and other people think is complete rubbish, that's the sign of a really good idea. 
    * **Never stop programming**.
    ![never-stop-programming](/content/images/2018/05/never-stop-programming.jpg)
* advice for new grad students
One good piece of advice for new grad students is, see if you can find an advisor who has beliefs similar to yours. Because if you work on stuff that your advisor feels deeply about, you'll get a lot of good advice and time from your advisor. ï¼ˆè¿™æ˜¾ç„¶å°±æ˜¯è‡ªå·±çš„äº²èº«ç»å†ï¼‰
*  I kind of agree with you, **that it's not quite a second industrial revolution, but it's something on nearly that scale**. And there's a huge sea change going on, basically because **our relationship to computers has changed. Instead of programming them, we now show them, and they figure it out**.
*  symbolic AI
And so I think thoughts are just these great big vectors, and that big vectors have causal powers. They cause other big vectors, and that's utterly unlike the standard AI view that thoughts are symbolic expressions.  