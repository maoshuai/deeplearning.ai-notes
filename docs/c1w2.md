

Basics of Neural Network Programming
# 1- Logistic Regression as a Neural Network

## 1.1- Binary Classification
1. Example: ç»™ä¸€å¼ 64x64åƒç´ çš„å›¾ç‰‡å›¾ç‰‡ï¼Œåˆ¤æ–­æ˜¯å¦å«æœ‰çŒ«
2. è·å–å›¾ç‰‡çš„RGBåƒç´ å€¼
![Screen-Shot-2018-06-02-at-17.20.31](/content/images/2018/06/Screen-Shot-2018-06-02-at-17.20.31.jpg)
3. å¹¶unrollæˆä¸€ä¸ªvector \\(X^{(i)}\\)
![Screen-Shot-2018-06-02-at-17.20.42](/content/images/2018/06/Screen-Shot-2018-06-02-at-17.20.42.jpg)
4. æ‰€æœ‰çš„vectorç»„æˆæ•°æ®é›†çŸ©é˜µ\\(X\\)
![Screen-Shot-2018-06-02-at-17.25.21](/content/images/2018/06/Screen-Shot-2018-06-02-at-17.25.21.jpg)
ç‰¹åˆ«æ³¨æ„ï¼Œ\\(X\\)çš„è¡Œæ˜¯\\(n\\)ï¼Œåˆ—æ˜¯\\(m\\)ï¼Œå’ŒMachine learningä¸­çš„å®šä¹‰æ­£å¥½æ˜¯è½¬ç½®çš„å…³ç³»ã€‚è¿™æ ·æœ‰ä¸ªå¥½å¤„ï¼Œæ¯æ¡æµ‹è¯•é›†åœ¨çŸ©é˜µä¸­éƒ½æ˜¯ä»¥åˆ—å‘é‡çš„å½¢å¼å­˜åœ¨ã€‚


5. DeepLearningå¸¸ç”¨Notations:
- \\(m\\): number of examples in datasets
- \\(n\_x\\): input sizeï¼ˆå³featureçš„ä¸ªæ•°ï¼‰
- \\(n\_y\\): output sizeï¼ˆå³åˆ†ç±»ä¸ªæ•°ï¼‰
- \\(X \\in \mathbb{R}^{n\_x\times m} \\) ï¼šthe input matrix
- \\(Y \\in \mathbb{R}^{n\_y\times m} \\) ï¼šthe input matrix
- å¸¦æ‹¬å·çš„ä¸Šæ ‡\\(^{(i)}\\)ï¼Œè¡¨ç¤ºå’Œtraining exampleç›¸å…³çš„è®¡æ•°

å®Œæ•´çš„notationï¼Œå¯ä»¥å‚è€ƒè¯¾ç¨‹ä¸­æä¾›çš„PDF: *Standard notations for Deep Learning*
![Screen-Shot-2018-06-10-at-21.21.45](/content/images/2018/06/Screen-Shot-2018-06-10-at-21.21.45.jpg)
![Screen-Shot-2018-06-10-at-21.21.58](/content/images/2018/06/Screen-Shot-2018-06-10-at-21.21.58.jpg)


6. ä½¿ç”¨Pythonä¸­çš„reshapeæ–¹æ³•ï¼Œæ•´ç†çŸ©é˜µçš„ç»´åº¦ã€‚

## 1.2- Logistic Regression
1. é—®é¢˜æè¿°ï¼š
Logsitic Regressionè¦æ±‚è¾“å‡ºyä¸æ˜¯0å°±æ˜¯1ã€‚The goal of logistic regression is to minimize the error between its predictions and training data.

![Screen-Shot-2018-06-02-at-17.37.35](/content/images/2018/06/Screen-Shot-2018-06-02-at-17.37.35.jpg)
2. sigmoid function

![Screen-Shot-2018-06-02-at-17.45.47](/content/images/2018/06/Screen-Shot-2018-06-02-at-17.45.47.jpg)

è¿™é‡Œæœ‰ä¸ªç–‘é—®ï¼Œä¸ºä»€ä¹ˆsigmoidå¤„ç†åçš„å€¼ï¼Œå¯ä»¥ä»£è¡¨y=1æ¦‚ç‡ï¼Ÿ

å‚è€ƒï¼š[Logistic distribution](https://stats.stackexchange.com/questions/80611/problem-understanding-the-logistic-regression-link-function/80623#80623)
2. å¼•å…¥å‚æ•°w, bï¼Œå…¶å®å°±æ˜¯Machine learningä¸­ç”¨çš„æ˜¯Î¸ï¼Œä½†DeepLearningä¸­åˆ†åˆ«ç”¨wå’Œbè¡¨ç¤ºã€‚å…¶ä¸­wæ˜¯vectorï¼Œbæ˜¯real number
4. è¿™é‡Œ ğ‘¦Ì‚ å°±æ˜¯Machine learningé‡Œé¢çš„hypothesis function: h(Î¸)



## 1.2- Logistic Regression Cost Funciton

1. Loss (error) functionçš„å®šä¹‰ï¼š
![Screen-Shot-2018-06-02-at-17.52.42](/content/images/2018/06/Screen-Shot-2018-06-02-at-17.52.42.jpg)
åˆ†æˆyä¸º0å’Œ1ä¸¤ç§æƒ…å†µå»ç†è§£è¿™ä¸ªå‡½æ•°ï¼Œ**æœ¬è´¨ä¸Šå°±æ˜¯å¯¹\\(\hat y\\)åšå¯¹æ•°å¤„ç†**è€Œå·²ã€‚
å› ä¸ºå¯¹æ•°å¤„ç†åç¡®å®**è¾¾åˆ°äº†Loss functionçš„è¦æ±‚**ï¼ˆæˆ‘è‡ªå·±çš„ç†è§£ï¼‰ï¼š1.å€¼åŸŸæ˜¯å¤§äºç­‰äº0çš„å®æ•°é›†ã€‚ 2. éšç€ğ‘¦Ì‚ å•è°ƒé€’å‡ã€‚y=ğ‘¦Ì‚ çš„æ—¶å€™ä¸º0ï¼Œåä¹‹è¶‹å‘äºâˆã€‚3. æ˜¯å‚æ•°çš„å‡¸å‡½æ•°ï¼ˆconvexï¼‰4. æ˜¯yå’Œğ‘¦Ì‚çš„å‡½æ•° 

2. æ²¡æœ‰ä½¿ç”¨square errorï¼Œå› ä¸ºæ˜¯non-convexï¼Œæ— æ³•ä½¿ç”¨Gradient Descentç®—æ³•
3. Loss functionæ˜¯é’ˆå¯¹å•ä¸ªtraining exampleçš„ï¼Œè€Œ**Cost functionæ˜¯Loss Functionçš„åœ¨æ‰€æœ‰training exampleä¸Šçš„å‡å€¼**ã€‚
![Screen-Shot-2018-06-02-at-18.03.20](/content/images/2018/06/Screen-Shot-2018-06-02-at-18.03.20.jpg)
åœ¨Machine learningé‡Œï¼Œæ²¡æœ‰å¼•å…¥Loss Functionï¼Œå…¶å®æœ‰ä¸€ä¸ªLoss Functionï¼Œæ›´å¥½ç†è§£ã€‚

## 1.3- Gradient Descent
Gradient Descentçš„åŸç†ï¼ˆIntuitionï¼‰ï¼šæŒ‰æ¢¯åº¦æœ€å¤§çš„æ–¹å‘é€¼è¿‘æœ€å°å€¼ã€‚
![Screen-Shot-2018-06-02-at-20.48.07](/content/images/2018/06/Screen-Shot-2018-06-02-at-20.48.07.jpg)

Gradient Descentç®—æ³•æ­¥éª¤ï¼š
1. Initialize \\(w\\), \\(b\\) to zero
2. repeatï¼š

$$ w\ :=w - w\frac{\partial J( w,b)}{\partial w}$$
$$ b\ :=b - b\frac{\partial J( w,b)}{\partial b}$$

## 1.4- Derivatives
ä¸ºä¸äº†è§£å¯¼æ•°çš„äººä»‹ç»å¯¼æ•°çš„ç›´è§‚å«ä¹‰ï¼Œè¿™é‡Œä¸ä½œè¯´æ˜äº†ã€‚

## 1.5- More Derivative Examples
ä¸ºä¸äº†è§£å¯¼æ•°çš„äººä»‹ç»å¯¼æ•°çš„ç›´è§‚å«ä¹‰ï¼Œè¿™é‡Œä¸ä½œè¯´æ˜äº†ã€‚

## 1.6- Computation graph
ä»å·¦åˆ°å³ï¼šè®¡ç®—å‡½æ•°J
ä»å³åˆ°å·¦ï¼šè®¡ç®—Jå¯¹å‚æ•°wå’Œbå¯¼æ•°

## 1.7- Derivative with a Computation Graph
* å…¶å®æ˜¯å°±æ˜¯å¤åˆå‡½æ•°çš„é“¾å¼æ³•åˆ™ã€‚
* è®¡ç®—å›¾å·¦è¾¹çš„å˜é‡çš„åå¯¼æ•°ä¾èµ–äºå³è¾¹çš„åå¯¼æ•°ï¼Œå³è¾¹çš„åå¯¼æ•°è®¡ç®—åï¼Œå¯ä»¥è¢«å·¦è¾¹çš„è®¡ç®—å¤ç”¨ã€‚

åœ¨Pythonä¸­è¡¨ç¤ºåå¯¼æ•°
$$dvar = \frac{\partial J}{\partial var}$$

## 1.8- Logistic Regression Gradient Descent
ä½¿ç”¨Computation Graphè®¡ç®—

![Screen-Shot-2018-06-04-at-08.26.07](/content/images/2018/06/Screen-Shot-2018-06-04-at-08.26.07.jpg)

è™½ç„¶ï¼Œæµ‹è¯•é›†æ˜¯ç¦»æ•£çš„ï¼Œä½†å¹¶ä¸ä»£è¡¨å¯¹wçš„å€’æ•°æ˜¯ç¦»æ•£çš„ï¼Œè¿™ä¸¤è€…æ²¡æœ‰ä»»ä½•å…³ç³»ã€‚**å§‹ç»ˆæ³¨æ„ï¼šåœ¨gradient Descentçš„æ—¶å€™ï¼Œxæ˜¯å¸¸é‡**

## 1.9- Gradient Descent on m Examples
Cost Functionçš„åå¯¼æ˜¯Loss Functionåå¯¼çš„å‡å€¼ï¼š

$$\frac{\partial J(w,b)}{\partial w\_j} =\frac{1}{m} \sum\_{i=1}^{m} \frac{\partial \mathcal{L}(a^{(i)},y^{(i)})}{\partial w\_j^{(i)}}$$

Gradient Descentç®—æ³•è¿‡ç¨‹ï¼š
1. æ±‚å¯¼è¿‡ç¨‹
 æ±‚å¯¼è¿‡ç¨‹åˆé€šå¸¸æ˜¯å…ˆforward propagationæ±‚cost functionï¼Œç„¶åå†backward propagationæ±‚åˆ°wå’Œbçš„å€’æ•°
3. ä¸‹é™è¿‡ç¨‹
ä½¿ç”¨åˆ°wå’Œbçš„å¯¼æ•°ï¼Œè¿­ä»£åšæ¢¯åº¦ä¸‹é™è¿‡ç¨‹ã€‚

ä¸‹é¢çš„æˆªå›¾å°±æ˜¯ä¸€ä¸ªéå‘é‡åŒ–çš„å®ç°ï¼š
å·¦è¾¹æ˜¯æ±‚å¯¼è¿‡ç¨‹ï¼Œå³è¾¹æ˜¯æ¢¯åº¦ä¸‹é™è¿‡ç¨‹
![Screen-Shot-2018-06-04-at-09.02.27](/content/images/2018/06/Screen-Shot-2018-06-04-at-09.02.27.jpg)


# 2- Python and Vectorization
## 2.1- Vectorization
1. ä»€ä¹ˆæ˜¯Vectorizationï¼šå°† for loop å°½å¯èƒ½è½¬æ¢ä¸ºçŸ©é˜µè¿ç®—ã€‚ä¸¾ä¾‹ï¼š
$$z = w^Tx + b$$
![Screen-Shot-2018-06-04-at-09.13.25](/content/images/2018/06/Screen-Shot-2018-06-04-at-09.13.25.jpg)

2. vectorizationçš„å¥½å¤„ï¼šconciser code, but faster execution
ä¸€ä¸ªç®€å•çš„å¯¹æ¯”å®éªŒï¼š1,000,000å¤§å°çš„ä¸¤ä¸ªå‘é‡å†…ç§¯è®¡ç®—ï¼Œfor loopè¦æ¯”Vectorizationå¿«300å€ã€‚
åœ¨DeepLearningæ—¶ä»£ï¼Œvectorizationæ˜¯ä¸€é¡¹é‡è¦çš„æŠ€èƒ½ã€‚

3. SIMD
Both CPU and GPU have parallelization instructions(i.e. SIMD, Signle Instruction Multiple Data)

## 2.2- More vectorization Examples
1. åŸåˆ™ï¼šwhenever possible, avoid explict for-loops
2. ä½¿ç”¨Element wisedçš„çŸ©é˜µè¿ç®—ï¼Œå°†å‡½æ•°ä½œç”¨åœ¨æ¯ä¸ªçŸ©é˜µå…ƒç´ ä¸Šï¼Œæ¯”å¦‚ï¼š
    * np.exp()
    * np.log()
    * np.abs()
    * np.maxium()
    * 1/v
    * v\*\*2

## 2.3- Vectorizing logistic Regression

![Screen-Shot-2018-06-04-at-19.50.07](/content/images/2018/06/Screen-Shot-2018-06-04-at-19.50.07.jpg)

$$A = \sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$$

ï¼ˆæˆ‘å§‹ç»ˆè§‰å¾—Aåº”è¯¥å°å†™ï¼Œæ¯•ç«Ÿè¿˜æ˜¯ä¸€ä¸ªè¡Œå‘é‡ï¼‰

ä¸ªäººç»éªŒï¼š
1. é¦–å…ˆï¼Œç†Ÿæ‚‰æ¯ä¸ªå˜é‡çš„è®°å·å’Œç»´åº¦ï¼Œå¿…è¦çš„è¯ï¼Œå¯ä»¥ç”»å‡ºæ¥ï¼Œæ›´ç›´è§‚ã€‚
2. å…ˆä»ä¸€ä¸ªæ ·æœ¬åšå‘é‡åŒ–ï¼Œå†æŠŠmä¸ªæ ·æœ¬çš„æ“ä½œå‘é‡åŒ–ã€‚
3. for-loopé‡Œé¢æ˜¯å¾ªç¯ä¹˜æ³•ï¼Œåˆ™å‘é‡åŒ–ä¸€å®šæ˜¯ä¸€ä¸ªä¹˜æ³•å½¢å¼ï¼Œè‹¥å¯¹äºä¸ç¡®å®šä¹˜æ³•çš„å·¦å³å…³ç³»ï¼Œæ˜¯å¦éœ€è½¬ç½®ï¼Œå¯ä»¥æ ¹æ®ç›®æ ‡å˜é‡çš„ç»´åº¦æ¨æµ‹ã€‚æˆ–è€…å…ˆä¹˜èµ·æ¥ï¼Œå†æ ¹æ®ç›®æ ‡å˜é‡çœ‹æ˜¯å¦è¦è½¬ç½®ã€‚

## 2.4- Vectorizing Logistic Regression's Gradient Output

æ¨å¯¼è¿‡ç¨‹

![Screen-Shot-2018-06-09-at-18.35.35](/content/images/2018/06/Screen-Shot-2018-06-09-at-18.35.35.jpg)

æœ€ç»ˆå‘é‡åŒ–çš„å½¢å¼æ˜¯ï¼š

$$ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T$$
$$ \frac{\partial J}{\partial b} = \frac{1}{m} \sum\_{i=1}^m (a^{(i)}-y^{(i)})$$

## 2.5- Broadcasting in Python
åœ¨matlabå’ŒPythonä¸­ï¼Œéƒ½é»˜è®¤æ”¯æŒä¸åŒç»´åº¦çš„å˜é‡åšelement-wisedçš„è®¡ç®—ï¼ˆ+-x/ç­‰ï¼‰ã€‚æ‰€è°“**Broadcasting**å…¶å®å°±æ˜¯é«˜çº¬åº¦æ•°ç»„å’Œä½çº¬åº¦æ•°ç»„è®¡ç®—æ—¶ï¼Œå°†ä½ç»´çš„å˜é‡**é€šè¿‡å¤åˆ¶çš„æ–¹å¼å‘é«˜ç»´æ‰©å±•ç»´åº¦**ï¼Œå†åšè¿ç®—ã€‚ä½†è¦æ³¨æ„ï¼š
1. ä½çº¬åº¦æ•°ç»„ä¸é«˜çº¬åº¦å…±æœ‰çš„ç»´åº¦ï¼Œå…ƒç´ ä¸ªæ•°å¿…é¡»ä¸€æ ·ï¼Œæ¯”å¦‚ä¸€ä¸ªshapeæ˜¯(5,3)çš„æ•°ç»„å¯ä»¥å’Œä¸€ä¸ªshapeæ˜¯(5,)çš„æ•°ç»„ç›¸åŠ ï¼Œä½†ä¸èƒ½å’Œä¸€ä¸ª(4,)çš„æ•°ç»„ç›¸åŠ ã€‚
2. ç»´åº¦ç›¸åŒçš„æ•°ç»„çš„æ•°ç»„ï¼Œæ˜¯ä¸èƒ½broadcastingçš„ï¼Œæ¯”å¦‚ä¸€ä¸ªshapeæ˜¯(5,3)çš„æ•°ç»„å’Œä¸€ä¸ªshapeæ˜¯(5,2)çš„æ•°ç»„è¿ç®—ï¼Œåè€…æ˜¯æ— æ³•broadcastingçš„
3. æŸä¸ªç»´åº¦çš„ä¸ªæ•°æ˜¯1ï¼Œç­‰åŒäºè¿™ä¸ªç»´åº¦ä¸å­˜åœ¨ï¼Œå¯ä»¥broadcastingï¼Œæ¯”å¦‚shapeæ˜¯(5,3)å’Œ(5,1)çš„æ•°ç»„å¯ä»¥è¿ç®—ã€‚

æ€»çš„æ¥è¯´ï¼Œå°±æ˜¯broadcastingè¦åšæŸä¸ªç»´åº¦çš„å¤åˆ¶ï¼Œå¿…é¡»åœ¨èµ‹å€¼çš„æ—¶å€™è¡Œå¾—é€šã€‚æ¯”å¦‚shapeæ˜¯(5,3)å’Œshapeæ˜¯(5,2)çš„æ•°ç»„è¿ç®—ï¼Œåè€…è¦å°†2å¤åˆ¶ä¸º3æ˜¯è¡Œä¸é€šçš„ï¼Œå› ä¸ºå­˜åœ¨ä¸¤è¡Œï¼Œé‚£å–å“ªä¸€è¡Œï¼Ÿ

Andrewçš„ä¸€ä¸ªç»éªŒï¼šå¦‚æœå¯¹æŸä¸ªæ•°ç»„çš„shapeä¸ç¡®å®šï¼Œå¯ä»¥ç”¨reshapeæ˜¾å¼çš„è°ƒç”¨ä¸€ä¸‹ï¼Œç¡®ä¿ç»´åº¦æ­£ç¡®ã€‚

è¡¥å……ï¼šnumpyä¸­ï¼Œç±»ä¼¼sumçš„å‡½æ•°ï¼Œç»å¸¸æ¶‰åŠaxiså‚æ•°ï¼Œå¯ä»¥å–å€¼ä¸º0æˆ–1ï¼Œç”šè‡³å…¶ä»–ã€‚ç»å¸¸è®°ä¸ä½ï¼Œè¿™é‡Œæˆ‘æŸ¥äº†äº†ä¸€ä¸‹ï¼Œæ˜¯è¿™æ ·çš„ï¼ˆ[åŸæ–‡](https://stackoverflow.com/questions/17079279/how-is-axis-indexed-in-numpys-array)ï¼‰ï¼š
1. **axisçš„æ•°å­—ï¼Œå’Œæ•°ç»„çš„shapeå‚æ•°çš„ç´¢å¼•æ˜¯å¯¹åº”çš„**ã€‚æ¯”å¦‚ä¸€ä¸ªæ•°ç»„çš„shapeæ˜¯(5,6)ï¼Œåˆ™ä»£è¡¨5ä¸ªrowï¼Œ6ä¸ªcolumnã€‚å³åœ¨shapeä¸­ï¼Œrowå’Œcolumnçš„ä¸ªæ•°çš„ç´¢å¼•æ˜¯0å’Œ1ã€‚ä¹Ÿå°±ç¬¬1ä¸ªåæ ‡ï¼Œåœ¨shapeä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œ**ç´¢å¼•æ˜¯0ï¼Œä»£è¡¨rowçš„æ–¹å‘**ï¼›ç¬¬2ä¸ªåæ ‡ï¼Œåœ¨shapeä¸­çš„ç¬¬2ä¸ªå…ƒç´ ï¼Œ**ç´¢å¼•æ˜¯1ï¼Œä»£è¡¨rowçš„æ–¹å‘**ã€‚
2. å¯¹äºsumå‡½æ•°ï¼ŒaxisæŒ‡çš„æ˜¯sumâ€œ**æ²¿ç€**â€çš„æ–¹å‘ï¼Œç»è¿‡è®¡ç®—ï¼Œè¿™ä¸ªæ–¹å‘çš„ç»´åº¦å› ä¸ºæ±‚å’Œåå°±æ¶ˆå¤±äº†ï¼Œæ¯”å¦‚sum(axis=0)ä»£è¡¨æ˜¯æ²¿ç€â€œrowâ€æ–¹å‘è¿›è¡Œæ±‚å’Œï¼Œ
3. å½“ç„¶axiså¯ä»¥æ˜¯ä¸€ä¸ªtupeï¼Œé‚£å°±ç›¸å½“äºæ²¿ç€å¤šä¸ªå¤šä¸ªæ–¹å‘æ±‚å’Œã€‚
4. sumå¦‚æœä¸ä¼ å…¥axiså‚æ•°ï¼Œé»˜è®¤æ˜¯å¯¹æ‰€æœ‰ç»´åº¦æ±‚å’Œã€‚

## 2.6- A note on python/numpy vectors
broadcastingçš„ä¸€ä¸ªå¼±ç‚¹ï¼šå¯èƒ½éšè—æ½œåœ¨çš„é”™è¯¯ï¼Œæ¯”å¦‚ä¸€ä¸ªè®¡ç®—ä¸­æœ¬æ¥è¦å»ä¸¤ä¸ªè¿ç®—çš„æ•°ç»„ç»´åº¦ä¸€æ ·ï¼Œå¦‚æœæ²¡æœ‰broadcastingï¼Œå°±ä¼šç›´æ¥æŠ¥é”™ï¼›è€Œbroadcastingå…è®¸å¯ç»§ç»­æ‰§è¡Œã€‚

**rank 1 arrayé—®é¢˜**ï¼šshapeæ˜¯(x,)çš„æ•°ç»„ï¼Œæ—¢ä¸æ˜¯è¡Œå‘é‡ï¼Œä¹Ÿä¸æ˜¯åˆ—å‘é‡ï¼Œæ²¡æ³•å‚ä¸æ­£å¸¸çš„çŸ©é˜µè¿ç®—ï¼Œåº”è¯¥æ€»æ˜¯ä½¿ç”¨(x,1)æˆ–(1,x)çš„shapeæ¥è¡¨ç¤ºå‘é‡ã€‚ä½†å¯ä»¥é€šè¿‡reshapeæ–¹æ³•å°†rank 1 arrayè½¬æ¢ä¸ºè¡Œå‘é‡æˆ–åˆ—å‘é‡ã€‚ï¼ˆä»€ä¹ˆæ˜¯rankï¼Œå°±æ˜¯ä¸€ä¸ªæ•°ç»„çš„ç»´åº¦ï¼‰

![Screen-Shot-2018-06-10-at-18.03.37](/content/images/2018/06/Screen-Shot-2018-06-10-at-18.03.37.jpg)

## 2.7- Quick tour of Jupyter/iPython Notebooks

è¿™éƒ¨åˆ†ï¼Œä¹Ÿå¯ä»¥å‚è€ƒæˆ‘ä¹‹å‰çš„æ–‡ç« ï¼šã€Š[Jupyter Notebookç®€ä»‹å’Œé…ç½®è¯´æ˜](//imshuai.com/jupyter-notebook-introduction-configuration/)ã€‹

## 2.8- Explanation of logistic regression cost function (optional)
è®¡ç®—ç»“æœğ‘¦Ì‚ä»£è¡¨äº†ç»™å®šæ ·æœ¬xï¼Œy=1çš„æ¦‚ç‡ï¼Œå³ğ‘¦Ì‚=P(y=1|x)

ä½†ä¸ºä»€ä¹ˆå¯ä»¥å¯¹åº”åˆ°æ¦‚ç‡å‘¢ï¼Ÿå‚è€ƒï¼šä¸ºä»€ä¹ˆsigmoidå¯ä»¥ä»£è¡¨æ¦‚ç‡ï¼Œæ¶‰åŠLogistic distribution
https://stats.stackexchange.com/questions/80611/problem-understanding-the-logistic-regression-link-function/80623#80623

Loss functionå…¶å®å°±æ˜¯å¯¹æ¦‚ç‡P(y|x)å–å¯¹æ•°ï¼š

![Screen-Shot-2018-06-10-at-18.28.41](/content/images/2018/06/Screen-Shot-2018-06-10-at-18.28.41.jpg)

Loss functionè¶Šå°ï¼Œåˆ™å–åˆ°å’Œå®é™…å€¼yçš„æ¦‚ç‡è¶Šå¤§ã€‚

æ‰€æœ‰æ ·æœ¬çš„Cost functionï¼š

![Screen-Shot-2018-06-10-at-18.32.40](/content/images/2018/06/Screen-Shot-2018-06-10-at-18.32.40.jpg)

# 3- Heros of Deep Learningï¼šPieter Abbeel interview

![Screen-Shot-2018-06-10-at-18.34.23](/content/images/2018/06/Screen-Shot-2018-06-10-at-18.34.23.jpg)

1. Pieter Abbeelä¸“æ³¨äºdeep reinforcement learning.

2. advice for people entrying AI
    * A good time to enter AI. High demand.
    * Not just read things or watch videos but **try things out**.
    * With frameworks like **TensorFlow, Chainer, Theano, PyTorch and so forth**, it's very easy to get going and get something up and running very quickly.

3. Andrew Ng: We live in good times. If people want to learn. 

4. what are the things that deep reinforcement learning is already working really well at?
    * learning to play games from pixels
    * robot inventing walking, running, standing up with a single algorithm. 





